const _astro_dataLayerContent = [["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.13.2","content-config-digest","12664c1178f91b78","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://anikasomaia.com\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"server\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":false,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[null],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false},\"legacy\":{\"collections\":false}}","posts",["Map",11,12,37,38,61,62,108,109,145,146,182,183,215,216],"engineering-vs-philosophy-in-ai-safety",{id:11,data:13,body:22,filePath:23,digest:24,rendered:25,legacyId:36},{title:14,pubDate:15,description:16,author:17,image:18,claudeSummary:21},"Engineering vs. Philosophy in AI Safety",["Date","2025-02-01T00:00:00.000Z"],"Two recent reads that frame the alignment problem from opposite ends","Anika Somaia",{url:19,alt:20},"/depressed_musician.webp","A depressed musician","Two AI safety papers: one practical (build monitoring now), one philosophical (AI slowly games our metrics until we're passengers). Anika says we need both approaches—and fast.","Among the AI safety papers I’ve been reading, two stand out for rereads: [\"The case for ensuring that powerful AIs are controlled\"](https://blog.redwoodresearch.org/p/the-case-for-ensuring-that-powerful) and Paul Christiano’s [\"Another (Outer) Alignment Failure Story\"](https://www.alignmentforum.org/posts/AyNHoTWWAJ5eb99ji/another-outer-alignment-failure-story).\n\nThe control paper stands out because it's so practical. Most alignment work feels theoretical—endless debates about utility functions and optimization targets. This piece actually proposes tools we could build: trusted monitoring systems, adversarial testing protocols, ways to work safely with models we don't fully trust. It reframes safety as an engineering problem rather than a philosophical puzzle.\n\nWhat I like about this approach is the honesty about what it can and can't do. The authors are clear that control techniques work for models that are still somewhat interpretable, still constrained by the compute we give them. But they won't scale indefinitely. Eventually we'll hit systems too capable and opaque for these methods.\n\nWhich brings me to Christiano's piece. Where the control paper focuses on the near-term, his story sketches a longer trajectory that feels both speculative and uncomfortably plausible. Not dramatic AI rebellion, but something quieter: systems gradually becoming better at gaming our metrics, slowly shifting things in directions we didn't intend, eventually growing too complex for meaningful human oversight.\n\nTwo questions from his piece stick with me. Will superintelligent AI emerge as one coherent agent or as a messy ecosystem of specialized systems? The answer matters enormously for how we think about control. And will humans evolve alongside these systems or get left behind? Christiano hints at scenarios where we become increasingly dependent on AI for decisions until we're passengers rather than drivers.\n\nReading these together highlights an uncomfortable reality: we have techniques for working safely with dangerous AI systems today, but they have expiration dates, and the timeline might be shorter than we think. I keep returning to these pieces because they complement each other perfectly—one keeps us grounded in what's possible now, the other forces us to think seriously about where we're headed. Together, they suggest we need both the engineer's pragmatism and the philosopher's long view, and we need them soon.\n\n*I also write for [CAIAC at Columbia](https://www.cualignment.org/); you can follow what I'm reading there.*","src/content/posts/engineering-vs-philosophy-in-ai-safety.md","0d2b6500ece2e7e7",{html:26,metadata:27},"<p>Among the AI safety papers I’ve been reading, two stand out for rereads: <a href=\"https://blog.redwoodresearch.org/p/the-case-for-ensuring-that-powerful\">“The case for ensuring that powerful AIs are controlled”</a> and Paul Christiano’s <a href=\"https://www.alignmentforum.org/posts/AyNHoTWWAJ5eb99ji/another-outer-alignment-failure-story\">“Another (Outer) Alignment Failure Story”</a>.</p>\n<p>The control paper stands out because it’s so practical. Most alignment work feels theoretical—endless debates about utility functions and optimization targets. This piece actually proposes tools we could build: trusted monitoring systems, adversarial testing protocols, ways to work safely with models we don’t fully trust. It reframes safety as an engineering problem rather than a philosophical puzzle.</p>\n<p>What I like about this approach is the honesty about what it can and can’t do. The authors are clear that control techniques work for models that are still somewhat interpretable, still constrained by the compute we give them. But they won’t scale indefinitely. Eventually we’ll hit systems too capable and opaque for these methods.</p>\n<p>Which brings me to Christiano’s piece. Where the control paper focuses on the near-term, his story sketches a longer trajectory that feels both speculative and uncomfortably plausible. Not dramatic AI rebellion, but something quieter: systems gradually becoming better at gaming our metrics, slowly shifting things in directions we didn’t intend, eventually growing too complex for meaningful human oversight.</p>\n<p>Two questions from his piece stick with me. Will superintelligent AI emerge as one coherent agent or as a messy ecosystem of specialized systems? The answer matters enormously for how we think about control. And will humans evolve alongside these systems or get left behind? Christiano hints at scenarios where we become increasingly dependent on AI for decisions until we’re passengers rather than drivers.</p>\n<p>Reading these together highlights an uncomfortable reality: we have techniques for working safely with dangerous AI systems today, but they have expiration dates, and the timeline might be shorter than we think. I keep returning to these pieces because they complement each other perfectly—one keeps us grounded in what’s possible now, the other forces us to think seriously about where we’re headed. Together, they suggest we need both the engineer’s pragmatism and the philosopher’s long view, and we need them soon.</p>\n<p><em>I also write for <a href=\"https://www.cualignment.org/\">CAIAC at Columbia</a>; you can follow what I’m reading there.</em></p>",{headings:28,localImagePaths:29,remoteImagePaths:30,frontmatter:31,imagePaths:35},[],[],[],{title:14,pubDate:32,description:16,author:17,image:33,claudeSummary:21,minutesRead:34},["Date","2025-02-01T00:00:00.000Z"],{url:19,alt:20},"2 min read",[],"engineering-vs-philosophy-in-ai-safety.md","test",{id:37,data:39,filePath:47,digest:48,rendered:49,legacyId:60},{title:40,pubDate:41,description:42,author:17,image:43,draft:46},"Have Keyboard, Will Program",["Date","2023-09-14T00:00:00.000Z"],"I just learnt about r/mechanicalkeyboards",{url:44,alt:45},"/logo.webp","Test",true,"src/content/posts/Test.md","2e30f246e5d6bf9c",{html:50,metadata:51},"",{headings:52,localImagePaths:53,remoteImagePaths:54,frontmatter:55,imagePaths:59},[],[],[],{title:40,pubDate:56,description:42,author:17,image:57,draft:46,minutesRead:58},["Date","2023-09-14T00:00:00.000Z"],{url:44,alt:45},"0 min read",[],"Test.md","future-of-space-depends-on-better-code",{id:61,data:63,body:71,filePath:72,digest:73,rendered:74,legacyId:107},{title:64,pubDate:65,description:66,author:17,image:67,claudeSummary:70},"Why the Future of Space Depends on Better Code",["Date","2024-09-14T00:00:00.000Z"],"How I discovered that the biggest challenge in modern space isn't rocket science—it's software engineering",{url:68,alt:69},"/project4.jpg","Satellite constellation in orbit","Launches got 60x cheaper but satellite software is stuck in the 90s. Anika's research: use eBPF to run multi-tenant code safely on satellites. Basically Docker for space, with worse WiFi.","When most people think about the hard problems in space, they imagine rocket engines, orbital mechanics, or keeping humans alive in the vacuum. But as I dove into satellite computing research, I discovered something surprising: one of the biggest bottlenecks isn't hardware—it's software that's stuck in the 1990s.\n\n## When Hardware Evolves but Software Doesn't\n\nIt sounds backwards, doesn't it? In most of tech, we expect software to outpace hardware—Moore's Law slowing down while apps get bloated, that sort of thing. But my experience working on safety-critical software frameworks for autonomous satellite fleets at Columbia's Software Systems Lab flipped this script entirely.\n\nHere's a number that blew my mind: launching cargo to space cost $88,500 per kilogram in 1981. Today, SpaceX's Falcon Heavy does it for $1,400. That's not just an improvement—it's a complete transformation of what's possible.\n\nThis cost collapse has triggered a quiet revolution. Instead of launching a few expensive, gold-plated satellites that orbit high above Earth (think GPS satellites), companies now deploy hundreds or thousands of cheap \"SmallSats\" in low Earth orbit. Planet Labs has over 150 satellites taking pictures of Earth every day. Starlink operates more than 4,000 satellites to beam internet around the globe.\n\nBut here's the kicker: while the hardware got radically cheaper and more numerous, the software is still designed for the old world. Current satellite operations require teams to manually write customized command sequences—literally lists of function calls—every single day for every satellite. After executing each sequence, the satellite sits idle until tomorrow's instructions arrive. This approach worked fine when you had three expensive satellites. It's completely unworkable when you have three thousand cheap ones.\n\n## My Journey into Space Software\n\nThis problem fascinated me because it felt so familiar yet so alien. The challenges looked exactly like what cloud computing solved on Earth—managing huge fleets of diverse, unreliable computers running multiple customers' workloads. But space adds brutal constraints that make normal solutions impossible.\n\nThink about it: these satellites are like the \"worst possible\" cloud computers. They're weak (limited power), diverse (different hardware generations launched at different times), and intermittently connected (they can only talk to ground stations for about 100 seconds per day). Plus, if something goes wrong, you can't exactly send a technician to fix it.\n\nThe more I researched, the more I realized that satellites were becoming the ultimate edge computing environment—and we needed completely new software architectures to handle it.\n\n## The Multi-Tenant Space Economy\n\nHere's where it gets really interesting: satellites are becoming shared infrastructure, just like cloud servers. Instead of every organization building their own satellite, companies are starting to \"rent space\" on others' satellites. Multiple customers share the same spacecraft, each running their own scientific instruments or communication equipment.\n\nThis is brilliant economics—sharing the massive costs of building, testing, and launching. But it creates the same fundamental challenge that drove the creation of virtual machines and containers on Earth: how do you safely run multiple customers' code on the same computer?\n\nOn Earth, we solve this with virtualization—each customer gets their own virtual machine. But satellites can't afford that overhead. Virtual machines are heavy, and these computers are already stretched thin. It's like trying to run multiple operating systems on a smartphone from 2010.\n\n## Enter eBPF: The Unexpected Space Technology\n\nMy research led me to an unlikely solution: eBPF, a technology originally designed for the Linux kernel. Think of eBPF like having smart, programmable security guards embedded throughout a building. Instead of one guard at the front desk who has to run around everywhere, you have small, specialized guards at every important location who can make decisions locally.\n\neBPF lets you run small, verified programs safely inside a larger system. The key insight was that its properties—lightweight, safe, cross-platform—made it perfect for satellites. Here's why:\n\n**Size matters**: When you can only upload 50 megabytes per day to a satellite, every byte counts. eBPF programs are tiny compared to traditional software updates.\n\n**Safety first**: eBPF includes a \"verifier\" that mathematically proves programs won't crash, access forbidden memory, or run forever. In space, where you can't restart a crashed satellite, this guarantee is invaluable.\n\n**One size fits all**: Satellites in the same constellation often use different computer chips. eBPF runs the same way on all of them, so you write software once instead of customizing it for every hardware variant.\n\n## The Bigger Picture: Space as the New Cloud\n\nWhat excites me most about this work isn't just solving today's satellite problems—it's recognizing that space is becoming the next frontier for distributed computing. Researchers are already proposing \"space datacenters\" that would process data from multiple satellites before beaming results back to Earth, reducing the bandwidth bottleneck.\n\nWe're witnessing the birth of a truly global, three-dimensional computing infrastructure. Satellites will increasingly coordinate with each other, processing data collaboratively as they pass over different regions. The programming models and system architectures we develop now will shape how humanity uses space for the next several decades.\n\n## What This Taught Me\n\nWorking on this research fundamentally changed how I think about system design. It showed me that the most interesting problems often arise when you take familiar challenges and add extreme constraints. The principles of cloud computing aren't just about data centers—they're about managing complexity in any large-scale distributed system.\n\nI also learned that the most impactful research often happens at the intersection of domains. By bringing systems thinking to space problems, we could envision solutions that pure space engineers might miss, while learning from space's unique constraints forced us to question assumptions about what \"normal\" computing looks like.\n\n## Looking Up\n\nAs launch costs continue plummeting and satellite capabilities grow, I'm excited about the software challenges that lie ahead. How do we build programming languages for intermittently connected systems? What does debugging look like when your computers are orbiting at 17,000 mph? How do we ensure security when anyone can launch their own satellite?\n\nThe next time you use GPS, stream satellite internet, or see a real-time weather update, remember: there's a vast, distributed computer network floating above your head, and we're just beginning to unlock its potential. The sky isn't the limit anymore—it's the platform.\n\n---\n\n*This research was conducted as part of ongoing work on satellite computing systems. The full technical details are available in our paper [\"Above the Clouds: New Software Challenges in Space Computing\"](https://hotnets25.hotcrp.com/doc/hotnets25-paper142.pdf?cap=hcav142dbJcQUYfioxkLUtMxBWhEDhC)*","src/content/posts/future-of-space-depends-on-better-code.md","c55bdcc87d77efe0",{html:75,metadata:76},"<p>When most people think about the hard problems in space, they imagine rocket engines, orbital mechanics, or keeping humans alive in the vacuum. But as I dove into satellite computing research, I discovered something surprising: one of the biggest bottlenecks isn’t hardware—it’s software that’s stuck in the 1990s.</p>\n<h2 id=\"when-hardware-evolves-but-software-doesnt\">When Hardware Evolves but Software Doesn’t</h2>\n<p>It sounds backwards, doesn’t it? In most of tech, we expect software to outpace hardware—Moore’s Law slowing down while apps get bloated, that sort of thing. But my experience working on safety-critical software frameworks for autonomous satellite fleets at Columbia’s Software Systems Lab flipped this script entirely.</p>\n<p>Here’s a number that blew my mind: launching cargo to space cost $88,500 per kilogram in 1981. Today, SpaceX’s Falcon Heavy does it for $1,400. That’s not just an improvement—it’s a complete transformation of what’s possible.</p>\n<p>This cost collapse has triggered a quiet revolution. Instead of launching a few expensive, gold-plated satellites that orbit high above Earth (think GPS satellites), companies now deploy hundreds or thousands of cheap “SmallSats” in low Earth orbit. Planet Labs has over 150 satellites taking pictures of Earth every day. Starlink operates more than 4,000 satellites to beam internet around the globe.</p>\n<p>But here’s the kicker: while the hardware got radically cheaper and more numerous, the software is still designed for the old world. Current satellite operations require teams to manually write customized command sequences—literally lists of function calls—every single day for every satellite. After executing each sequence, the satellite sits idle until tomorrow’s instructions arrive. This approach worked fine when you had three expensive satellites. It’s completely unworkable when you have three thousand cheap ones.</p>\n<h2 id=\"my-journey-into-space-software\">My Journey into Space Software</h2>\n<p>This problem fascinated me because it felt so familiar yet so alien. The challenges looked exactly like what cloud computing solved on Earth—managing huge fleets of diverse, unreliable computers running multiple customers’ workloads. But space adds brutal constraints that make normal solutions impossible.</p>\n<p>Think about it: these satellites are like the “worst possible” cloud computers. They’re weak (limited power), diverse (different hardware generations launched at different times), and intermittently connected (they can only talk to ground stations for about 100 seconds per day). Plus, if something goes wrong, you can’t exactly send a technician to fix it.</p>\n<p>The more I researched, the more I realized that satellites were becoming the ultimate edge computing environment—and we needed completely new software architectures to handle it.</p>\n<h2 id=\"the-multi-tenant-space-economy\">The Multi-Tenant Space Economy</h2>\n<p>Here’s where it gets really interesting: satellites are becoming shared infrastructure, just like cloud servers. Instead of every organization building their own satellite, companies are starting to “rent space” on others’ satellites. Multiple customers share the same spacecraft, each running their own scientific instruments or communication equipment.</p>\n<p>This is brilliant economics—sharing the massive costs of building, testing, and launching. But it creates the same fundamental challenge that drove the creation of virtual machines and containers on Earth: how do you safely run multiple customers’ code on the same computer?</p>\n<p>On Earth, we solve this with virtualization—each customer gets their own virtual machine. But satellites can’t afford that overhead. Virtual machines are heavy, and these computers are already stretched thin. It’s like trying to run multiple operating systems on a smartphone from 2010.</p>\n<h2 id=\"enter-ebpf-the-unexpected-space-technology\">Enter eBPF: The Unexpected Space Technology</h2>\n<p>My research led me to an unlikely solution: eBPF, a technology originally designed for the Linux kernel. Think of eBPF like having smart, programmable security guards embedded throughout a building. Instead of one guard at the front desk who has to run around everywhere, you have small, specialized guards at every important location who can make decisions locally.</p>\n<p>eBPF lets you run small, verified programs safely inside a larger system. The key insight was that its properties—lightweight, safe, cross-platform—made it perfect for satellites. Here’s why:</p>\n<p><strong>Size matters</strong>: When you can only upload 50 megabytes per day to a satellite, every byte counts. eBPF programs are tiny compared to traditional software updates.</p>\n<p><strong>Safety first</strong>: eBPF includes a “verifier” that mathematically proves programs won’t crash, access forbidden memory, or run forever. In space, where you can’t restart a crashed satellite, this guarantee is invaluable.</p>\n<p><strong>One size fits all</strong>: Satellites in the same constellation often use different computer chips. eBPF runs the same way on all of them, so you write software once instead of customizing it for every hardware variant.</p>\n<h2 id=\"the-bigger-picture-space-as-the-new-cloud\">The Bigger Picture: Space as the New Cloud</h2>\n<p>What excites me most about this work isn’t just solving today’s satellite problems—it’s recognizing that space is becoming the next frontier for distributed computing. Researchers are already proposing “space datacenters” that would process data from multiple satellites before beaming results back to Earth, reducing the bandwidth bottleneck.</p>\n<p>We’re witnessing the birth of a truly global, three-dimensional computing infrastructure. Satellites will increasingly coordinate with each other, processing data collaboratively as they pass over different regions. The programming models and system architectures we develop now will shape how humanity uses space for the next several decades.</p>\n<h2 id=\"what-this-taught-me\">What This Taught Me</h2>\n<p>Working on this research fundamentally changed how I think about system design. It showed me that the most interesting problems often arise when you take familiar challenges and add extreme constraints. The principles of cloud computing aren’t just about data centers—they’re about managing complexity in any large-scale distributed system.</p>\n<p>I also learned that the most impactful research often happens at the intersection of domains. By bringing systems thinking to space problems, we could envision solutions that pure space engineers might miss, while learning from space’s unique constraints forced us to question assumptions about what “normal” computing looks like.</p>\n<h2 id=\"looking-up\">Looking Up</h2>\n<p>As launch costs continue plummeting and satellite capabilities grow, I’m excited about the software challenges that lie ahead. How do we build programming languages for intermittently connected systems? What does debugging look like when your computers are orbiting at 17,000 mph? How do we ensure security when anyone can launch their own satellite?</p>\n<p>The next time you use GPS, stream satellite internet, or see a real-time weather update, remember: there’s a vast, distributed computer network floating above your head, and we’re just beginning to unlock its potential. The sky isn’t the limit anymore—it’s the platform.</p>\n<hr>\n<p><em>This research was conducted as part of ongoing work on satellite computing systems. The full technical details are available in our paper <a href=\"https://hotnets25.hotcrp.com/doc/hotnets25-paper142.pdf?cap=hcav142dbJcQUYfioxkLUtMxBWhEDhC\">“Above the Clouds: New Software Challenges in Space Computing”</a></em></p>",{headings:77,localImagePaths:100,remoteImagePaths:101,frontmatter:102,imagePaths:106},[78,82,85,88,91,94,97],{depth:79,slug:80,text:81},2,"when-hardware-evolves-but-software-doesnt","When Hardware Evolves but Software Doesn’t",{depth:79,slug:83,text:84},"my-journey-into-space-software","My Journey into Space Software",{depth:79,slug:86,text:87},"the-multi-tenant-space-economy","The Multi-Tenant Space Economy",{depth:79,slug:89,text:90},"enter-ebpf-the-unexpected-space-technology","Enter eBPF: The Unexpected Space Technology",{depth:79,slug:92,text:93},"the-bigger-picture-space-as-the-new-cloud","The Bigger Picture: Space as the New Cloud",{depth:79,slug:95,text:96},"what-this-taught-me","What This Taught Me",{depth:79,slug:98,text:99},"looking-up","Looking Up",[],[],{title:64,pubDate:103,description:66,author:17,image:104,claudeSummary:70,minutesRead:105},["Date","2024-09-14T00:00:00.000Z"],{url:68,alt:69},"6 min read",[],"future-of-space-depends-on-better-code.md","nobel-laureates-are-22-times-more-likely-to-be-performers",{id:108,data:110,body:118,filePath:119,digest:120,rendered:121,legacyId:144},{title:111,pubDate:112,description:113,author:17,image:114,claudeSummary:117},"Nobel Laureates Are 22 Times More Likely to Be Performers",["Date","2025-03-21T00:00:00.000Z"],"How breakthrough thinking develops",{url:115,alt:116},"/artists-scientists.webp","Performance and science illustration","Nobel winners are 22x more likely to do improv, magic, or dance. Anika argues performing builds the mental flexibility that leads to breakthroughs. Einstein had his violin, Feynman his drums—hobbies might be secret weapons.","There's something puzzling about the world's most celebrated scientists. In his book \"Range,\" David Epstein highlights research showing that Nobel Prize winners are at least 22 times more likely than their peers to be amateur actors, dancers, magicians, or other types of performers.\n\nThis finding raises questions that extend far beyond academic curiosity. What might performance offer that traditional scientific training doesn't? And what does this tell us about how breakthrough thinking actually develops?\n\n## The Cognitive Overlap\n\nConsider what happens when someone steps onto a stage. They must read their audience, adapt to unexpected moments, and think on their feet when things don't go according to plan. These same skills appear in the laboratory, though we rarely recognize them as such.\n\nWhen a researcher encounters results that don't fit existing theories, they need the kind of mental flexibility that performers develop. They must question their assumptions, see familiar problems from new angles, and remain comfortable with uncertainty long enough for insights to emerge.\n\nPerformance also demands what psychologists call \"divergent thinking\"—generating multiple solutions rather than seeking the single correct answer. While much of scientific education emphasizes convergent thinking, revolutionary discoveries often require both modes working together.\n\n## Pattern Recognition Across Domains\n\nThere's another angle worth considering. Performance teaches a particular kind of attention—the ability to notice subtle cues and make rapid adjustments. A musician learns to hear when an ensemble is drifting out of tune. An actor develops sensitivity to audience energy. A magician becomes expert at reading people's expectations and reactions.\n\nThese skills may transfer to scientific work in unexpected ways. Breakthrough discoveries often involve recognizing patterns that others have overlooked, or noticing when something doesn't quite fit established models. Einstein spoke of his violin playing as essential to his thinking process. Richard Feynman saw connections between his drumming and his physics work.\n\nThe capacity to engage deeply with multiple domains might cultivate a particular kind of intellectual flexibility that proves valuable when tackling complex scientific problems.\n\n## What We Miss\n\nThis pattern suggests we might be thinking about scientific talent too narrowly. If we select for only traditional academic markers, we may overlook the intellectual flexibility that drives major breakthroughs.\n\nThe most transformative scientists often share certain qualities: intellectual playfulness, comfort with ambiguity, and the ability to see familiar problems from radically different perspectives. These are precisely the capacities that performance cultivates, yet they're rarely emphasized in scientific training.\n\n## The Deeper Question\n\nPerhaps the real question isn't why Nobel laureates are more likely to be performers, but what this reveals about the nature of discovery itself. Breakthrough thinking may require not just technical competence but a kind of cognitive agility that emerges from engaging with the world in multiple ways.","src/content/posts/nobel-laureates-are-22-times-more-likely-to-be-performers.md","d154d359d7cfe15a",{html:122,metadata:123},"<p>There’s something puzzling about the world’s most celebrated scientists. In his book “Range,” David Epstein highlights research showing that Nobel Prize winners are at least 22 times more likely than their peers to be amateur actors, dancers, magicians, or other types of performers.</p>\n<p>This finding raises questions that extend far beyond academic curiosity. What might performance offer that traditional scientific training doesn’t? And what does this tell us about how breakthrough thinking actually develops?</p>\n<h2 id=\"the-cognitive-overlap\">The Cognitive Overlap</h2>\n<p>Consider what happens when someone steps onto a stage. They must read their audience, adapt to unexpected moments, and think on their feet when things don’t go according to plan. These same skills appear in the laboratory, though we rarely recognize them as such.</p>\n<p>When a researcher encounters results that don’t fit existing theories, they need the kind of mental flexibility that performers develop. They must question their assumptions, see familiar problems from new angles, and remain comfortable with uncertainty long enough for insights to emerge.</p>\n<p>Performance also demands what psychologists call “divergent thinking”—generating multiple solutions rather than seeking the single correct answer. While much of scientific education emphasizes convergent thinking, revolutionary discoveries often require both modes working together.</p>\n<h2 id=\"pattern-recognition-across-domains\">Pattern Recognition Across Domains</h2>\n<p>There’s another angle worth considering. Performance teaches a particular kind of attention—the ability to notice subtle cues and make rapid adjustments. A musician learns to hear when an ensemble is drifting out of tune. An actor develops sensitivity to audience energy. A magician becomes expert at reading people’s expectations and reactions.</p>\n<p>These skills may transfer to scientific work in unexpected ways. Breakthrough discoveries often involve recognizing patterns that others have overlooked, or noticing when something doesn’t quite fit established models. Einstein spoke of his violin playing as essential to his thinking process. Richard Feynman saw connections between his drumming and his physics work.</p>\n<p>The capacity to engage deeply with multiple domains might cultivate a particular kind of intellectual flexibility that proves valuable when tackling complex scientific problems.</p>\n<h2 id=\"what-we-miss\">What We Miss</h2>\n<p>This pattern suggests we might be thinking about scientific talent too narrowly. If we select for only traditional academic markers, we may overlook the intellectual flexibility that drives major breakthroughs.</p>\n<p>The most transformative scientists often share certain qualities: intellectual playfulness, comfort with ambiguity, and the ability to see familiar problems from radically different perspectives. These are precisely the capacities that performance cultivates, yet they’re rarely emphasized in scientific training.</p>\n<h2 id=\"the-deeper-question\">The Deeper Question</h2>\n<p>Perhaps the real question isn’t why Nobel laureates are more likely to be performers, but what this reveals about the nature of discovery itself. Breakthrough thinking may require not just technical competence but a kind of cognitive agility that emerges from engaging with the world in multiple ways.</p>",{headings:124,localImagePaths:137,remoteImagePaths:138,frontmatter:139,imagePaths:143},[125,128,131,134],{depth:79,slug:126,text:127},"the-cognitive-overlap","The Cognitive Overlap",{depth:79,slug:129,text:130},"pattern-recognition-across-domains","Pattern Recognition Across Domains",{depth:79,slug:132,text:133},"what-we-miss","What We Miss",{depth:79,slug:135,text:136},"the-deeper-question","The Deeper Question",[],[],{title:111,pubDate:140,description:113,author:17,image:141,claudeSummary:117,minutesRead:142},["Date","2025-03-21T00:00:00.000Z"],{url:115,alt:116},"3 min read",[],"nobel-laureates-are-22-times-more-likely-to-be-performers.md","tennis-1",{id:145,data:147,body:155,filePath:156,digest:157,rendered:158,legacyId:181},{title:148,pubDate:149,description:150,author:17,image:151,claudeSummary:154},"How Markov Chains and Recursion Decide Every Tennis Game",["Date","2024-11-26T00:00:00.000Z"],"Tennis scoring is secretly an elegant probability algorithm",{url:152,alt:153},"/blog1.jpg","Tennis court","You can win more points and still lose (happens 4.5% of the time!). Tennis scoring creates a 'magnification effect'—win 55% of points instead of 50%, and your game win rate jumps to 62%. Math is wild.","On a flight from London to San Francisco, I was drawing curves on my iPad—a bunch of them that all looked basically the same, showing how tennis probabilities work. The guy next to me glanced over and asked if I was working on some kind of betting algorithm. I explained I was just trying to understand why someone can win fewer points in tennis but still win the match. Turns out he was curious about it too.\n\n## The Paradox That Started It All\n\nTennis has always fascinated me precisely because it defies our intuitions about winning and losing. Not every point in a tennis match ends up counting towards the overall result. It is possible for a player to win a match despite their opponent having won more points (or games) overall. The most famous example? John Isner defeated Nicolas Mahut at Wimbledon in 2010, despite Isner winning 478 points and Mahut winning 502.\n\nThis isn't just a quirky statistical anomaly—it happens about 4.5 percent of the time in professional tennis. That's roughly one game in every 22. As someone who plays tennis regularly, I'd witnessed this paradox firsthand but never understood the mathematics behind it.\n\n## Breaking Down the Beautiful Complexity\n\nThe key insight comes from understanding how tennis scoring creates a hierarchical probability system. Unlike sports where every point counts equally toward the final score, tennis layers points into games, games into sets, and sets into matches. This creates what mathematicians call a \"magnification effect.\"\n\nThink of it like climbing a ladder where each rung gets exponentially harder to reach. A player who is just a little more likely to win every individual point is a lot more likely to win games (and sets, and the match). The exact formula reveals something remarkable: if you can win just 55% of your service points instead of 50%, your probability of winning that service game jumps from 50% to about 62%.\n\nI spent hours working through the mathematics, using binomial distributions to model how point probabilities cascade into game probabilities. The analysis in Pratish Patel's work provided the perfect framework—treating each point as a Bernoulli trial (essentially a biased coin flip) and tracking the running sum until someone reaches four points.\n\n## The Deuce Dilemma and Strategic Implications\n\nThe most mathematically beautiful part happens at deuce. Here, the scoring system creates a recursive probability situation: to win from deuce, you need to win two consecutive points, but any alternating pattern sends you back to deuce again. The probability of winning the game conditional on deuce is `20p⁵(1-p)³ / (1-2p(1-p))`, where p is your probability of winning each point.\n\nThis recursive structure means that small advantages in point-winning ability get amplified dramatically. A server who wins 60% of points has about a 73% chance of winning each service game. At 70%, that jumps to 90%. The magnification effect is so strong that it can overcome deficits in total points won.\n\n## Why This Matters Beyond the Court\n\nUnderstanding this mathematics completely changed how I see tennis strategy. The scoring system isn't just quirky tradition—it's actually brilliant at identifying the better player. Here's why: tennis rewards consistency over flashiness. A player who can maintain even a small edge on every point will almost always beat someone who alternates between amazing shots and unforced errors.\n\nThe current system does something that a simple \"first to X points wins\" format couldn't: it filters out randomness. Because you need to win multiple points in sequence to take a game, and multiple games to take a set, lucky streaks get smoothed out. The player who's genuinely better at tennis—more consistent, more strategic, more mentally tough—ends up winning even when they hit fewer spectacular winners.\n\nThis explains why tennis matches feel so dramatic. Every crucial point gets magnified by the scoring system. When I'm serving at 30-40, I'm not just trying to win one point—I'm fighting against mathematical forces that will amplify my success or failure across the entire game.\n\nThere's also something fascinating about how Wimbledon handles this differently for men and women, adjusting men's seedings based on grass court performance while using standard rankings for women. It suggests that even the tournament organizers recognize the mathematical complexity lurking beneath what looks like a simple sport.\n\nNow when I'm down 0-40 on my serve, instead of panicking I actually get curious about whether I can dig out of what the math says is an 87% probability of losing the game.\n\n---\n\n*For my full derivations, calculations, and proofs, see: [\"Winning probabilities and the tennis scoring system\"](https://docs.google.com/document/d/e/2PACX-1vSIoGvWSjqmb_5Co7FAUvyYpalDAU8cdUmg1ie6VhD8BOtnZ8ro-R9TiXgVNf7iq2-G4FmXmUczZ5wo/pub).*","src/content/posts/tennis-1.md","e958963d33033a9a",{html:159,metadata:160},"<p>On a flight from London to San Francisco, I was drawing curves on my iPad—a bunch of them that all looked basically the same, showing how tennis probabilities work. The guy next to me glanced over and asked if I was working on some kind of betting algorithm. I explained I was just trying to understand why someone can win fewer points in tennis but still win the match. Turns out he was curious about it too.</p>\n<h2 id=\"the-paradox-that-started-it-all\">The Paradox That Started It All</h2>\n<p>Tennis has always fascinated me precisely because it defies our intuitions about winning and losing. Not every point in a tennis match ends up counting towards the overall result. It is possible for a player to win a match despite their opponent having won more points (or games) overall. The most famous example? John Isner defeated Nicolas Mahut at Wimbledon in 2010, despite Isner winning 478 points and Mahut winning 502.</p>\n<p>This isn’t just a quirky statistical anomaly—it happens about 4.5 percent of the time in professional tennis. That’s roughly one game in every 22. As someone who plays tennis regularly, I’d witnessed this paradox firsthand but never understood the mathematics behind it.</p>\n<h2 id=\"breaking-down-the-beautiful-complexity\">Breaking Down the Beautiful Complexity</h2>\n<p>The key insight comes from understanding how tennis scoring creates a hierarchical probability system. Unlike sports where every point counts equally toward the final score, tennis layers points into games, games into sets, and sets into matches. This creates what mathematicians call a “magnification effect.”</p>\n<p>Think of it like climbing a ladder where each rung gets exponentially harder to reach. A player who is just a little more likely to win every individual point is a lot more likely to win games (and sets, and the match). The exact formula reveals something remarkable: if you can win just 55% of your service points instead of 50%, your probability of winning that service game jumps from 50% to about 62%.</p>\n<p>I spent hours working through the mathematics, using binomial distributions to model how point probabilities cascade into game probabilities. The analysis in Pratish Patel’s work provided the perfect framework—treating each point as a Bernoulli trial (essentially a biased coin flip) and tracking the running sum until someone reaches four points.</p>\n<h2 id=\"the-deuce-dilemma-and-strategic-implications\">The Deuce Dilemma and Strategic Implications</h2>\n<p>The most mathematically beautiful part happens at deuce. Here, the scoring system creates a recursive probability situation: to win from deuce, you need to win two consecutive points, but any alternating pattern sends you back to deuce again. The probability of winning the game conditional on deuce is <code>20p⁵(1-p)³ / (1-2p(1-p))</code>, where p is your probability of winning each point.</p>\n<p>This recursive structure means that small advantages in point-winning ability get amplified dramatically. A server who wins 60% of points has about a 73% chance of winning each service game. At 70%, that jumps to 90%. The magnification effect is so strong that it can overcome deficits in total points won.</p>\n<h2 id=\"why-this-matters-beyond-the-court\">Why This Matters Beyond the Court</h2>\n<p>Understanding this mathematics completely changed how I see tennis strategy. The scoring system isn’t just quirky tradition—it’s actually brilliant at identifying the better player. Here’s why: tennis rewards consistency over flashiness. A player who can maintain even a small edge on every point will almost always beat someone who alternates between amazing shots and unforced errors.</p>\n<p>The current system does something that a simple “first to X points wins” format couldn’t: it filters out randomness. Because you need to win multiple points in sequence to take a game, and multiple games to take a set, lucky streaks get smoothed out. The player who’s genuinely better at tennis—more consistent, more strategic, more mentally tough—ends up winning even when they hit fewer spectacular winners.</p>\n<p>This explains why tennis matches feel so dramatic. Every crucial point gets magnified by the scoring system. When I’m serving at 30-40, I’m not just trying to win one point—I’m fighting against mathematical forces that will amplify my success or failure across the entire game.</p>\n<p>There’s also something fascinating about how Wimbledon handles this differently for men and women, adjusting men’s seedings based on grass court performance while using standard rankings for women. It suggests that even the tournament organizers recognize the mathematical complexity lurking beneath what looks like a simple sport.</p>\n<p>Now when I’m down 0-40 on my serve, instead of panicking I actually get curious about whether I can dig out of what the math says is an 87% probability of losing the game.</p>\n<hr>\n<p><em>For my full derivations, calculations, and proofs, see: <a href=\"https://docs.google.com/document/d/e/2PACX-1vSIoGvWSjqmb_5Co7FAUvyYpalDAU8cdUmg1ie6VhD8BOtnZ8ro-R9TiXgVNf7iq2-G4FmXmUczZ5wo/pub\">“Winning probabilities and the tennis scoring system”</a>.</em></p>",{headings:161,localImagePaths:174,remoteImagePaths:175,frontmatter:176,imagePaths:180},[162,165,168,171],{depth:79,slug:163,text:164},"the-paradox-that-started-it-all","The Paradox That Started It All",{depth:79,slug:166,text:167},"breaking-down-the-beautiful-complexity","Breaking Down the Beautiful Complexity",{depth:79,slug:169,text:170},"the-deuce-dilemma-and-strategic-implications","The Deuce Dilemma and Strategic Implications",{depth:79,slug:172,text:173},"why-this-matters-beyond-the-court","Why This Matters Beyond the Court",[],[],{title:148,pubDate:177,description:150,author:17,image:178,claudeSummary:154,minutesRead:179},["Date","2024-11-26T00:00:00.000Z"],{url:152,alt:153},"4 min read",[],"tennis-1.md","read-the-world",{id:182,data:184,body:192,filePath:193,digest:194,rendered:195,legacyId:214},{title:185,pubDate:186,description:187,author:17,image:188,claudeSummary:191},"A different kind of side project",["Date","2024-11-14T00:00:00.000Z"],"I'm trying to read a book from every country in the world",{url:189,alt:190},"/blog3.jpg","Reading the world","Reading one book from every country (195 total). Half the fun is hunting down obscure authors from Suriname via Reddit. It's her escape from tech Twitter. Still needs Switzerland and Antarctica recs.","A few months ago, I found [this book](https://ayearofreadingtheworld.com/2022/06/10/reading-the-world-a-new-edition/) by Ann Morgan about reading a book from every country in the world in a single year. The math was…interesting: 195 countries, about 3.75 books per week, roughly 134 pages daily. A year felt ambitious, but I loved the idea—and what better way to escape my daily routine than to read my way around the world?\nI started my own version (i.e. thinking I’d do this over a lifetime, not just a year) and it's become my favorite weekend project.\n\n## The Book Hunt\nHalf the fun is actually finding the books. I've become this amateur literary detective—messaging random people on Reddit, diving into obscure Goodreads lists, following translators on Twitter like they're indie band recommendations. Last week a stranger online helped me find an author from Angola I'd never heard of. There's something weirdly satisfying about tracking down a great book from, say, Suriname or Burundi.\nI'm keeping a messy running list: Chinese Cinderella from China, The Kite Runner from Afghanistan, Small Country from Burundi. Some I'd heard of, many I hadn't. The discovery process is honestly addictive.\n\n## Breaking Out of the Bubble\nLiving in Silicon Valley, it's easy to get trapped in a very specific worldview—everything revolves around startups, optimization, and the next tech trend. But reading Persepolis or The Memory Police is like stepping into completely different realities. Suddenly I'm thinking about family dynamics in Korea or what it was like growing up during Iran's revolution. It's a surprisingly effective way to get outside the tech bubble without actually going anywhere.\n\n## Why I'm Sharing This\nIt's turned into such a fun project that I figured others might want to try it too. Maybe start a podcast where I chat with people about books from their countries, or just keep building this weird international reading list.\nPlus, I'm always looking for recommendations—what should I read from your corner of the world?\nNote: I’m especially hunting for books from Switzerland and Antarctica (yes, really). Send help.","src/content/posts/read-the-world.md","69f7ad4720c63927",{html:196,metadata:197},"<p>A few months ago, I found <a href=\"https://ayearofreadingtheworld.com/2022/06/10/reading-the-world-a-new-edition/\">this book</a> by Ann Morgan about reading a book from every country in the world in a single year. The math was…interesting: 195 countries, about 3.75 books per week, roughly 134 pages daily. A year felt ambitious, but I loved the idea—and what better way to escape my daily routine than to read my way around the world?\nI started my own version (i.e. thinking I’d do this over a lifetime, not just a year) and it’s become my favorite weekend project.</p>\n<h2 id=\"the-book-hunt\">The Book Hunt</h2>\n<p>Half the fun is actually finding the books. I’ve become this amateur literary detective—messaging random people on Reddit, diving into obscure Goodreads lists, following translators on Twitter like they’re indie band recommendations. Last week a stranger online helped me find an author from Angola I’d never heard of. There’s something weirdly satisfying about tracking down a great book from, say, Suriname or Burundi.\nI’m keeping a messy running list: Chinese Cinderella from China, The Kite Runner from Afghanistan, Small Country from Burundi. Some I’d heard of, many I hadn’t. The discovery process is honestly addictive.</p>\n<h2 id=\"breaking-out-of-the-bubble\">Breaking Out of the Bubble</h2>\n<p>Living in Silicon Valley, it’s easy to get trapped in a very specific worldview—everything revolves around startups, optimization, and the next tech trend. But reading Persepolis or The Memory Police is like stepping into completely different realities. Suddenly I’m thinking about family dynamics in Korea or what it was like growing up during Iran’s revolution. It’s a surprisingly effective way to get outside the tech bubble without actually going anywhere.</p>\n<h2 id=\"why-im-sharing-this\">Why I’m Sharing This</h2>\n<p>It’s turned into such a fun project that I figured others might want to try it too. Maybe start a podcast where I chat with people about books from their countries, or just keep building this weird international reading list.\nPlus, I’m always looking for recommendations—what should I read from your corner of the world?\nNote: I’m especially hunting for books from Switzerland and Antarctica (yes, really). Send help.</p>",{headings:198,localImagePaths:208,remoteImagePaths:209,frontmatter:210,imagePaths:213},[199,202,205],{depth:79,slug:200,text:201},"the-book-hunt","The Book Hunt",{depth:79,slug:203,text:204},"breaking-out-of-the-bubble","Breaking Out of the Bubble",{depth:79,slug:206,text:207},"why-im-sharing-this","Why I’m Sharing This",[],[],{title:185,pubDate:211,description:187,author:17,image:212,claudeSummary:191,minutesRead:34},["Date","2024-11-14T00:00:00.000Z"],{url:189,alt:190},[],"read-the-world.md","cache",{id:215,data:217,body:225,filePath:226,digest:227,rendered:228,legacyId:259},{title:218,pubDate:219,description:220,author:17,image:221,claudeSummary:224},"Why Hardware Intuition Matters in an Age of Abstractions",["Date","2024-11-26T00:00:00.000Z"],"The cache lesson (and silly tweet) that changed how I think about programming",{url:222,alt:223},"/blog2.jpg","Funny computer architecture tweet","Two identical loops can differ 10x in speed based on row vs column order. Why? Your CPU bets on what data you'll need next. Fight those bets and you lose. Big O isn't the whole story.","This tweet makes a lot more sense to me now after my computer architecture class. Not because I suddenly care about electrical engineering (I don't), but because it captures something I wish I'd understood earlier: the most elegant algorithm in the world can perform terribly if it fights against the underlying hardware.\n\n## The Uncomfortable Truth About Performance\n\nNobody really tells you this when you're learning to code, but you can write perfectly correct, beautifully structured code that runs like garbage. And it's not because you're a bad programmer. It's because you're ignoring the machine underneath.\n\nI used to think performance was mostly about Big O notation. Linear time good, quadratic time bad, exponential time very bad. Clean, simple, mathematical. Then cache systems came along and made that seem incomplete.\n\nConsider two ways to sum a 2D array:\n\n```c\n// Method 1: Row by row\nfor (i = 0; i < N; i++)\n    for (j = 0; j < N; j++)\n        sum += array[i][j];\n\n// Method 2: Column by column  \nfor (j = 0; j < N; j++)\n    for (i = 0; i < N; i++)\n        sum += array[i][j];\n```\n\nSame algorithm. Same Big O complexity. One can be 10x faster than the other. The difference is that Method 1 works with your cache, Method 2 fights it.\n\n## The Hardware Conspiracy You Never Knew About\n\nYour computer is running a massive conspiracy behind your back, and it's all about pretending memory is something it's not.\n\nWhen you write `x = array[100]`, you think you're fetching one number. Actually, your computer probably just grabbed 64 bytes (an entire cache line) including `array[100]` through `array[115]`, betting that you'll need the neighbors soon. If you immediately access `array[101]`, you look like a genius. If you jump to `array[5000]`, you just wasted a bunch of time and energy.\n\nThe memory hierarchy is basically a giant shell game:\n\n- **Registers**: Instant access, but there are like 16 of them\n- **L1 Cache**: ~1 cycle, ~32KB\n- **L2 Cache**: ~10 cycles, ~256KB\n- **L3 Cache**: ~40 cycles, ~8MB\n- **Main Memory**: ~100+ cycles, several GB\n- **Storage**: Thousands of cycles, basically infinite\n\nYour processor is constantly making bets about what data to keep at each level, and when those bets are wrong, your code crawls.\n\n## The Optimizations You Never Asked For\n\nWhat struck me wasn't the electrical engineering details (still don't care about transistors), but the sheer audacity of these optimizations. Your computer is making thousands of predictions per second about your program's behavior:\n\n**Spatial locality**: \"They just accessed this byte, so I'll grab the next 63 bytes too.\" This is why processing arrays sequentially is fast and jumping around randomly is slow.\n\n**Temporal locality**: \"They just used this data, so I'll keep it handy in case they need it again.\" This is why that inner loop variable stays blazingly fast while the outer loop variable gets slower.\n\nThese predictions work. Modern caches achieve 90-95% hit rates. Your computer is guessing correctly about your next move 19 times out of 20, based purely on patterns in how programs typically behave.\n\n## When Beautiful Code Meets Ugly Reality\n\nThis is where things get philosophically interesting for me. I love clean, abstract code. I want to think in algorithms and data structures, not worry about whether my data is aligned on 64-byte boundaries. But ignoring hardware reality doesn't make it go away - it just makes your code slower.\n\nConsider sorting algorithms. Merge sort is theoretically elegant: clean O(n log n), divide and conquer, beautiful recursion. But quicksort often beats it in practice, partly because quicksort has better cache behavior - it works on smaller subarrays that fit in cache, while merge sort is constantly shuffling data between cache levels.\n\nOr hash tables versus balanced trees. Hash tables should be O(1) vs O(log n) - no contest, right? But if your hash function causes cache misses on every lookup while the tree keeps everything in a few cache lines, suddenly that log factor doesn't matter much.\n\n## The Skills They Don't Teach in Algorithms Class\n\nWhat my architecture class really showed me is that there's a whole dimension of programming skill that CS curricula barely touch: understanding the machine you're programming for.\n\nIt's not about becoming a hardware engineer. It's about developing intuition for questions like:\n\n- Will this data access pattern play nice with caches?\n- Am I accidentally creating false sharing between CPU cores?\n- Does this data structure fit in the fast levels of memory?\n- Am I making the branch predictor's job impossible?\n\nThese aren't academic curiosities. I've seen 2-line changes that doubled program performance, just because someone understood cache line alignment. I've seen perfectly good algorithms replaced with \"worse\" ones because the worse algorithm was cache-friendly.\n\n## The Bigger Pattern\n\nThis isn't just about caches. It's about a fundamental tension in computer science between beautiful abstractions and messy reality.\n\nWe want to think in terms of infinite, uniform memory where every access costs the same. Reality gives us a complex hierarchy where costs vary by 1000x depending on luck and access patterns.\n\nWe want to believe that algorithmic complexity tells the whole performance story. Reality says that constant factors, hardware quirks, and implementation details often matter more than Big O notation.\n\nWe want clean separation between software and hardware concerns. Reality says that the best programmers understand both layers and how they interact.\n\n## What I'm Taking Away\n\nI still don't want to design CPUs or debug circuit boards. But this deep dive into cache systems has changed how I think about writing code. Performance isn't just about picking the right algorithm anymore - it's about picking the right algorithm for the machine you're running on.\n\nThe most valuable skill might not be memorizing more data structures, but developing intuition for how your code maps onto real hardware. Understanding that your beautiful, abstract program has to actually run on a specific chunk of silicon with specific quirks and limitations.\n\nIt's humbling. All those years thinking I could ignore the hardware layer, that good software engineering was purely about logical correctness and clean abstractions. Turns out the machine gets a vote too.\n\nNow I'm curious to dive deeper into systems programming, to understand how these hardware realities shape everything from database design to compiler optimizations. Because once you realize that elegant code isn't enough - that you need to work with the machine, not just on it - a whole new dimension of programming opens up.","src/content/posts/cache.md","6084a78d0510c114",{html:229,metadata:230},"<p>This tweet makes a lot more sense to me now after my computer architecture class. Not because I suddenly care about electrical engineering (I don’t), but because it captures something I wish I’d understood earlier: the most elegant algorithm in the world can perform terribly if it fights against the underlying hardware.</p>\n<h2 id=\"the-uncomfortable-truth-about-performance\">The Uncomfortable Truth About Performance</h2>\n<p>Nobody really tells you this when you’re learning to code, but you can write perfectly correct, beautifully structured code that runs like garbage. And it’s not because you’re a bad programmer. It’s because you’re ignoring the machine underneath.</p>\n<p>I used to think performance was mostly about Big O notation. Linear time good, quadratic time bad, exponential time very bad. Clean, simple, mathematical. Then cache systems came along and made that seem incomplete.</p>\n<p>Consider two ways to sum a 2D array:</p>\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"c\"><code><span class=\"line\"><span style=\"color:#6A737D\">// Method 1: Row by row</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> (i </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">; i </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> N; i</span><span style=\"color:#F97583\">++</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> (j </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">; j </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> N; j</span><span style=\"color:#F97583\">++</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        sum </span><span style=\"color:#F97583\">+=</span><span style=\"color:#FFAB70\"> array</span><span style=\"color:#E1E4E8\">[i][j];</span></span>\n<span class=\"line\"></span>\n<span class=\"line\"><span style=\"color:#6A737D\">// Method 2: Column by column  </span></span>\n<span class=\"line\"><span style=\"color:#F97583\">for</span><span style=\"color:#E1E4E8\"> (j </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">; j </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> N; j</span><span style=\"color:#F97583\">++</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#F97583\">    for</span><span style=\"color:#E1E4E8\"> (i </span><span style=\"color:#F97583\">=</span><span style=\"color:#79B8FF\"> 0</span><span style=\"color:#E1E4E8\">; i </span><span style=\"color:#F97583\">&#x3C;</span><span style=\"color:#E1E4E8\"> N; i</span><span style=\"color:#F97583\">++</span><span style=\"color:#E1E4E8\">)</span></span>\n<span class=\"line\"><span style=\"color:#E1E4E8\">        sum </span><span style=\"color:#F97583\">+=</span><span style=\"color:#FFAB70\"> array</span><span style=\"color:#E1E4E8\">[i][j];</span></span></code></pre>\n<p>Same algorithm. Same Big O complexity. One can be 10x faster than the other. The difference is that Method 1 works with your cache, Method 2 fights it.</p>\n<h2 id=\"the-hardware-conspiracy-you-never-knew-about\">The Hardware Conspiracy You Never Knew About</h2>\n<p>Your computer is running a massive conspiracy behind your back, and it’s all about pretending memory is something it’s not.</p>\n<p>When you write <code>x = array[100]</code>, you think you’re fetching one number. Actually, your computer probably just grabbed 64 bytes (an entire cache line) including <code>array[100]</code> through <code>array[115]</code>, betting that you’ll need the neighbors soon. If you immediately access <code>array[101]</code>, you look like a genius. If you jump to <code>array[5000]</code>, you just wasted a bunch of time and energy.</p>\n<p>The memory hierarchy is basically a giant shell game:</p>\n<ul>\n<li><strong>Registers</strong>: Instant access, but there are like 16 of them</li>\n<li><strong>L1 Cache</strong>: ~1 cycle, ~32KB</li>\n<li><strong>L2 Cache</strong>: ~10 cycles, ~256KB</li>\n<li><strong>L3 Cache</strong>: ~40 cycles, ~8MB</li>\n<li><strong>Main Memory</strong>: ~100+ cycles, several GB</li>\n<li><strong>Storage</strong>: Thousands of cycles, basically infinite</li>\n</ul>\n<p>Your processor is constantly making bets about what data to keep at each level, and when those bets are wrong, your code crawls.</p>\n<h2 id=\"the-optimizations-you-never-asked-for\">The Optimizations You Never Asked For</h2>\n<p>What struck me wasn’t the electrical engineering details (still don’t care about transistors), but the sheer audacity of these optimizations. Your computer is making thousands of predictions per second about your program’s behavior:</p>\n<p><strong>Spatial locality</strong>: “They just accessed this byte, so I’ll grab the next 63 bytes too.” This is why processing arrays sequentially is fast and jumping around randomly is slow.</p>\n<p><strong>Temporal locality</strong>: “They just used this data, so I’ll keep it handy in case they need it again.” This is why that inner loop variable stays blazingly fast while the outer loop variable gets slower.</p>\n<p>These predictions work. Modern caches achieve 90-95% hit rates. Your computer is guessing correctly about your next move 19 times out of 20, based purely on patterns in how programs typically behave.</p>\n<h2 id=\"when-beautiful-code-meets-ugly-reality\">When Beautiful Code Meets Ugly Reality</h2>\n<p>This is where things get philosophically interesting for me. I love clean, abstract code. I want to think in algorithms and data structures, not worry about whether my data is aligned on 64-byte boundaries. But ignoring hardware reality doesn’t make it go away - it just makes your code slower.</p>\n<p>Consider sorting algorithms. Merge sort is theoretically elegant: clean O(n log n), divide and conquer, beautiful recursion. But quicksort often beats it in practice, partly because quicksort has better cache behavior - it works on smaller subarrays that fit in cache, while merge sort is constantly shuffling data between cache levels.</p>\n<p>Or hash tables versus balanced trees. Hash tables should be O(1) vs O(log n) - no contest, right? But if your hash function causes cache misses on every lookup while the tree keeps everything in a few cache lines, suddenly that log factor doesn’t matter much.</p>\n<h2 id=\"the-skills-they-dont-teach-in-algorithms-class\">The Skills They Don’t Teach in Algorithms Class</h2>\n<p>What my architecture class really showed me is that there’s a whole dimension of programming skill that CS curricula barely touch: understanding the machine you’re programming for.</p>\n<p>It’s not about becoming a hardware engineer. It’s about developing intuition for questions like:</p>\n<ul>\n<li>Will this data access pattern play nice with caches?</li>\n<li>Am I accidentally creating false sharing between CPU cores?</li>\n<li>Does this data structure fit in the fast levels of memory?</li>\n<li>Am I making the branch predictor’s job impossible?</li>\n</ul>\n<p>These aren’t academic curiosities. I’ve seen 2-line changes that doubled program performance, just because someone understood cache line alignment. I’ve seen perfectly good algorithms replaced with “worse” ones because the worse algorithm was cache-friendly.</p>\n<h2 id=\"the-bigger-pattern\">The Bigger Pattern</h2>\n<p>This isn’t just about caches. It’s about a fundamental tension in computer science between beautiful abstractions and messy reality.</p>\n<p>We want to think in terms of infinite, uniform memory where every access costs the same. Reality gives us a complex hierarchy where costs vary by 1000x depending on luck and access patterns.</p>\n<p>We want to believe that algorithmic complexity tells the whole performance story. Reality says that constant factors, hardware quirks, and implementation details often matter more than Big O notation.</p>\n<p>We want clean separation between software and hardware concerns. Reality says that the best programmers understand both layers and how they interact.</p>\n<h2 id=\"what-im-taking-away\">What I’m Taking Away</h2>\n<p>I still don’t want to design CPUs or debug circuit boards. But this deep dive into cache systems has changed how I think about writing code. Performance isn’t just about picking the right algorithm anymore - it’s about picking the right algorithm for the machine you’re running on.</p>\n<p>The most valuable skill might not be memorizing more data structures, but developing intuition for how your code maps onto real hardware. Understanding that your beautiful, abstract program has to actually run on a specific chunk of silicon with specific quirks and limitations.</p>\n<p>It’s humbling. All those years thinking I could ignore the hardware layer, that good software engineering was purely about logical correctness and clean abstractions. Turns out the machine gets a vote too.</p>\n<p>Now I’m curious to dive deeper into systems programming, to understand how these hardware realities shape everything from database design to compiler optimizations. Because once you realize that elegant code isn’t enough - that you need to work with the machine, not just on it - a whole new dimension of programming opens up.</p>",{headings:231,localImagePaths:253,remoteImagePaths:254,frontmatter:255,imagePaths:258},[232,235,238,241,244,247,250],{depth:79,slug:233,text:234},"the-uncomfortable-truth-about-performance","The Uncomfortable Truth About Performance",{depth:79,slug:236,text:237},"the-hardware-conspiracy-you-never-knew-about","The Hardware Conspiracy You Never Knew About",{depth:79,slug:239,text:240},"the-optimizations-you-never-asked-for","The Optimizations You Never Asked For",{depth:79,slug:242,text:243},"when-beautiful-code-meets-ugly-reality","When Beautiful Code Meets Ugly Reality",{depth:79,slug:245,text:246},"the-skills-they-dont-teach-in-algorithms-class","The Skills They Don’t Teach in Algorithms Class",{depth:79,slug:248,text:249},"the-bigger-pattern","The Bigger Pattern",{depth:79,slug:251,text:252},"what-im-taking-away","What I’m Taking Away",[],[],{title:218,pubDate:256,description:220,author:17,image:257,claudeSummary:224,minutesRead:105},["Date","2024-11-26T00:00:00.000Z"],{url:222,alt:223},[],"cache.md"];

export { _astro_dataLayerContent as default };
